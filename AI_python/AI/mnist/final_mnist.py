# -*- coding: utf-8 -*-
"""final_mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dbq0sjrmTv0M7FuV2dp2QKojIn3hJS6L
"""

# Data Enhancement
from keras.models import model_from_json
from keras.layers import Flatten, BatchNormalization, Conv2D, MaxPool2D
import tensorflow as tf
from tensorflow import keras
from keras.layers import Dense, Dropout
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

target_size = 28 * 28
categories = 10
fix = 'drive/MyDrive/00AI/'

def showImageTable(images, name):
    for i in range(0, 5):
        for j in range(0, 5):
            plt.subplot(5, 5, (i*5+j)+1)
            showImage(images[(i*5+j)])
    # plt.savefig(f'./AI/src/mnist_plt/{name}')


def showImage(image):
    plt.imshow(image)


def preprocessInput(images):
    return images.reshape(len(images), 28, 28, 1)/255.0


def labelsCategorical(labels):
    return keras.utils.to_categorical(labels)

# 1. Get training data
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# 2. Make dataset to target size
train_images = preprocessInput(train_images)
test_images = preprocessInput(test_images)

# 3. Make labels to categorical label
train_labels = labelsCategorical(train_labels)
test_labels = labelsCategorical(test_labels)

print(train_labels.shape)
# *. Extend data length
new_images = np.repeat(train_images, 2, axis=0)
new_labels = np.repeat(train_labels, 2, axis=0)

print(new_images.shape)

# 4. Initialize a model
model = tf.keras.models.Sequential()
kernel_size = [64, 128, 256]
# 5. Add model layers

model.add(Conv2D(32, (3, 3), padding='same', input_shape=(28,28, 1)))
model.add(BatchNormalization())

for size in kernel_size:
    model.add(Conv2D(size , (3, 3), padding='same'))
    model.add(Conv2D(size , (3, 3), padding='same'))
    if(size == kernel_size[2]): model.add(Conv2D(size , (3, 3), padding='same'))
    model.add(MaxPool2D((2,2)))
    model.add(BatchNormalization())

model.add(Flatten())
model.add(Dense(units=512, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(units=512, activation='relu'))
model.add(Dense(units=categories, activation='softmax'))

model.summary()

batch_size = 32

# Define ImageDataGenerator
datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=25,     # 0 ~ 180
    zoom_range=0.25,
    width_shift_range=0.20,
    height_shift_range=0.20,
)

# img_iter = datagen.flow(train_images, train_labels, batch_size=batch_size)


# Apply new data on train_images
datagen.fit(new_images)

# x, y = img_iter.next()  # get next new data by batch_size
# print(x.shape, y.shape)

# showImageTable(old, fix+'old01.png'
# showImageTable(x, fix+'new01.png')

model.compile(optimizer='adam', loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(
          datagen.flow(new_images, new_labels, batch_size=batch_size),
          epochs=10,
          batch_size=batch_size,
          verbose=1,
          #  validation_split=0.2,
          validation_data=(test_images, test_labels),
          #  validation_freq=20
          )

model.compile(
              tf.keras.optimizers.Adam(learning_rate=0.0003),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(
          train_images, train_labels,
          epochs=3,
          batch_size=batch_size,
          verbose=1,
          #  validation_split=0.2,
          validation_data=(test_images, test_labels),
          #  validation_freq=20
          )

model.save(fix+'/final_mnist_model')

# Load Model
from tensorflow import keras
import tensorflow as tf
import numpy as np
import glob, os
import sys

hardMode = 1
print(hardMode, sys.argv)
imgSize = (28,28)
imgUtils = keras.preprocessing.image


# 3. Loading image
def loadScaleImg(path):
    return imgUtils.load_img(path, color_mode="grayscale", target_size=imgSize)

def predictImage(path):
    image = loadScaleImg(path)
    image = imgUtils.img_to_array(image).reshape(imgSize)

    x = image.reshape(1,28, 28, 1) / 255.0
    answer = np.argmax(model.predict(x))
    return answer

if hardMode:
    base_path = fix + '/src/hard_mnist/'
else :
    base_path = fix + '/src/mnist/'

images = sorted([f for f in glob.glob(os.path.join(base_path, '*.png'))])

# print(images)
acc = np.array([])
for i,image in enumerate(images):
    predict = predictImage(image)
    print(image)
    print(f'predict: {predict} answer: {i%10}')
    acc = np.append(acc, predict==i%10)
print(acc.reshape(int(len(acc)/10),10))
print(f'{round(len(acc[acc==True])/len(acc)*100,3)}%')

# best acc in 2th stage
# loss: 0.0155 - accuracy: 0.9955 - val_loss: 0.0158 - val_accuracy: 0.9954

# best mnist
# [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
#  [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]]
# 95.0%


# best hard_mnist
# [[0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]
#  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
#  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
#  [1. 1. 1. 1. 1. 1. 0. 1. 0. 1.]]
# 80.0%

